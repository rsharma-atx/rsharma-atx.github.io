<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Richa Sharma" />
    
    <link rel="shortcut icon" type="image/x-icon" href="../../img/favicon.ico">
    <title>Project 1 finished</title>
    <meta name="generator" content="Hugo 0.79.0" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../../css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">
      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="../../post/">BLOG</a></li>
        
        <li><a href="../../projects/">PROJECTS</a></li>
        
        <li><a href="../../resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="../../project/project1/">Project 1 finished</a></strong>
          </h3>
        </div>
 
<div class="blog-title">
          <h4>
         January 1, 0001 
            &nbsp;&nbsp;
            
          </h4>
        </div>

        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              
<link href="../../rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="../../rmarkdown-libs/anchor-sections/anchor-sections.js"></script>
<script src="../../rmarkdown-libs/kePrint/kePrint.js"></script>
<link href="../../rmarkdown-libs/lightable/lightable.css" rel="stylesheet" />


<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.1
## ✓ tidyr   1.1.1     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library (carData)
library(fivethirtyeight)</code></pre>
<pre><code>## Some larger datasets need to be installed separately, like senators and
## house_district_forecast. To install these, we recommend you install the
## fivethirtyeightdata package by running:
## install.packages(&#39;fivethirtyeightdata&#39;, repos =
## &#39;https://fivethirtyeightdata.github.io/drat/&#39;, type = &#39;source&#39;)</code></pre>
<pre class="r"><code>library(ggplot2)</code></pre>
<p>My first dataset I chose was the amount of Murder, Rape, and Assault Arrests made in each state along with the percent urban population each state's population makes up. My second dataset provides data regarding fatal collisions in each state and the percent of those drivers that were alcohol-impaired, speeding, not distracted, not involved in previous accidents, and number of drivers involved in fatal collisions. It also provided data regarding car insurance premiums and losses those companies incurred due to accidents. All of this data was found across each state. The data regarding fatal collisions was gathered from historic data and insurance company information, while the arrest data was gathered form the McNeil monograph and from the Statistical Abstracts from 1975. Since this data is from 1975, it might not be reflective of modern society statistics as many societal changes have incurred over time. I liked these datasets because it is interesting to see the various statistics they have about Arrests in each state and how they relate to each other. Does one state have on average more rape arrests only or do they have more in all three categories? I also like the bad_drivers dataset because you only really hear about drunk driving on the news so that is how you think about most fatal collisions. It is interesting to explore the different reasons behind fatal collisions to see how they compare against each other.</p>
<p>since my dataset was already tidy, I made it untidy with pivot_wider by pulling the values from 'num_driver' to make those the new columns, and put their corresponding values from 'perc_speeding' in their columns</p>
<pre class="r"><code>bad_drivers &lt;- bad_drivers
BDW &lt;- bad_drivers %&gt;% pivot_wider(names_from=&quot;num_drivers&quot;, values_from=&quot;perc_speeding&quot;)
head(BDW)</code></pre>
<pre><code>## # A tibble: 6 x 51
##   state perc_alcohol perc_not_distra… perc_no_previous insurance_premi… losses
##   &lt;chr&gt;        &lt;int&gt;            &lt;int&gt;            &lt;int&gt;            &lt;dbl&gt;  &lt;dbl&gt;
## 1 Alab…           30               96               80             785.   145.
## 2 Alas…           25               90               94            1053.   134.
## 3 Ariz…           28               84               96             899.   110.
## 4 Arka…           26               94               95             827.   142.
## 5 Cali…           28               91               89             878.   166.
## 6 Colo…           28               79               95             836.   140.
## # … with 45 more variables: `18.8` &lt;int&gt;, `18.1` &lt;int&gt;, `18.6` &lt;int&gt;,
## #   `22.4` &lt;int&gt;, `12` &lt;int&gt;, `13.6` &lt;int&gt;, `10.8` &lt;int&gt;, `16.2` &lt;int&gt;,
## #   `5.9` &lt;int&gt;, `17.9` &lt;int&gt;, `15.6` &lt;int&gt;, `17.5` &lt;int&gt;, `15.3` &lt;int&gt;,
## #   `12.8` &lt;int&gt;, `14.5` &lt;int&gt;, `15.7` &lt;int&gt;, `17.8` &lt;int&gt;, `21.4` &lt;int&gt;,
## #   `20.5` &lt;int&gt;, `15.1` &lt;int&gt;, `12.5` &lt;int&gt;, `8.2` &lt;int&gt;, `14.1` &lt;int&gt;,
## #   `9.6` &lt;int&gt;, `17.6` &lt;int&gt;, `16.1` &lt;int&gt;, `14.9` &lt;int&gt;, `14.7` &lt;int&gt;,
## #   `11.6` &lt;int&gt;, `11.2` &lt;int&gt;, `18.4` &lt;int&gt;, `12.3` &lt;int&gt;, `16.8` &lt;int&gt;,
## #   `23.9` &lt;int&gt;, `19.9` &lt;int&gt;, `18.2` &lt;int&gt;, `11.1` &lt;int&gt;, `19.4` &lt;int&gt;,
## #   `19.5` &lt;int&gt;, `11.3` &lt;int&gt;, `12.7` &lt;int&gt;, `10.6` &lt;int&gt;, `23.8` &lt;int&gt;,
## #   `13.8` &lt;int&gt;, `17.4` &lt;int&gt;</code></pre>
<p>To re-tidy my data, I used pivot_longer to retake those columns and put them into 1 column with its values representing the percent of driversinvolved in fatal accidents who were speeding. I then removed the NAs from that column to condense the dataset so that there weren't rows of the same state with empty values</p>
<pre class="r"><code>baddrive_fixed &lt;- BDW %&gt;% pivot_longer(7:51, names_to=&quot;amt_drivers&quot;, values_to=&quot;percent_speeding&quot;)
baddrive_fixed &lt;- baddrive_fixed %&gt;%filter(!is.na(percent_speeding))
head(baddrive_fixed)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   state perc_alcohol perc_not_distra… perc_no_previous insurance_premi… losses
##   &lt;chr&gt;        &lt;int&gt;            &lt;int&gt;            &lt;int&gt;            &lt;dbl&gt;  &lt;dbl&gt;
## 1 Alab…           30               96               80             785.   145.
## 2 Alas…           25               90               94            1053.   134.
## 3 Ariz…           28               84               96             899.   110.
## 4 Arka…           26               94               95             827.   142.
## 5 Cali…           28               91               89             878.   166.
## 6 Colo…           28               79               95             836.   140.
## # … with 2 more variables: amt_drivers &lt;chr&gt;, percent_speeding &lt;int&gt;</code></pre>
<p>Now I converted my amt_drivers column from a character column, into a numeric column</p>
<pre class="r"><code>baddrive_fixed$amt_drivers &lt;- as.numeric(as.character(baddrive_fixed$amt_drivers))</code></pre>
<p>Here, the original data set didn't have a state column, but rather had the row names as states, so here I converted the state row names into a column entitled 'state'</p>
<pre class="r"><code>USArrests &lt;- USArrests
d  &lt;- USArrests %&gt;% rownames_to_column(&quot;state&quot;)</code></pre>
<p>Now, I untidyed the dataset with pivot_wider and then retidyed the dataset using pivot_longer, and removed all the duplicate rows w NAs</p>
<pre class="r"><code>US_wide &lt;- d %&gt;% pivot_wider(names_from=&quot;state&quot;, values_from=&quot;Rape&quot;)
dim(US_wide)</code></pre>
<pre><code>## [1] 50 53</code></pre>
<pre class="r"><code>USA_fixed &lt;- US_wide %&gt;% pivot_longer(4:53, names_to=&quot;state&quot;, values_to=&quot;Rape&quot;) %&gt;% filter(!is.na(Rape))
USA_fixed &lt;- USA_fixed %&gt;% select(state,everything())</code></pre>
<p>At the end of my tidying, I just moved the state column from the end to the first column in the dataset. I did an inner join, joining both of my datasets on the common ID variable 'state'</p>
<pre class="r"><code>full &lt;- inner_join(USA_fixed,baddrive_fixed)</code></pre>
<pre><code>## Joining, by = &quot;state&quot;</code></pre>
<p>The USA_fixed dataset has 50 observations while the baddrive_fixed dataset has 51 observations, because it included District of Colombia. 1 observation was dropped since I did an inner join and the District of Colombia did not have corresponding data in the USA_fixed dataset. I chose to do an inner join because the state column matched pretty well in both datasets, other than the fact that baddrive_fixed contained an observation for District of Colombia and USA_fixed did not. Inner join then dropped that observation since it did not have corresponding values in the USA_fixed dataset, so I just dropped that observation as a whole so we could just look at the data for the 50 states. Since the only observation that was dropped was that of the District of Colombia, I don't foresee this skewing my data because we still have a relatively large dataset.</p>
<pre class="r"><code>full %&gt;% select(state, Rape) %&gt;% group_by(state) %&gt;% filter(Rape&gt;30) %&gt;% arrange(-Rape)</code></pre>
<pre><code>## # A tibble: 8 x 2
## # Groups:   state [8]
##   state       Rape
##   &lt;chr&gt;      &lt;dbl&gt;
## 1 Nevada      46  
## 2 Alaska      44.5
## 3 California  40.6
## 4 Colorado    38.7
## 5 Michigan    35.1
## 6 New Mexico  32.1
## 7 Florida     31.9
## 8 Arizona     31</code></pre>
<p>Here I used the select function to only look at the state and the Rape columns so that we can focus on the number of arrests that were due to rape for each state. I then grouped the dataset by state so that if there were multiple observations concerning the same state they would be grouped together. Then, I used the filter function to only look at the those states with number of rape arrests that were greater than 30 per 100,000 people. Then I arranged the rape column in ascending order, from those states with the highest number of rape arrests to those with the lowest number of rape arrests.</p>
<pre class="r"><code>full_mutate &lt;- full %&gt;% mutate(net_ins_profit = insurance_premiums-losses)</code></pre>
<p>Here, I added a new column indicating the net profit insurance companies made per state from their premiums minus the amount they lost due to car accidents.</p>
<pre class="r"><code>full_mutate %&gt;% summarize_at(c(&quot;Rape&quot;,&quot;insurance_premiums&quot;,&quot;percent_speeding&quot;), mean, na.rm=T)</code></pre>
<pre><code>## # A tibble: 1 x 3
##    Rape insurance_premiums percent_speeding
##   &lt;dbl&gt;              &lt;dbl&gt;            &lt;dbl&gt;
## 1  21.2               879.             31.7</code></pre>
<p>Here, I looked at the columns indicating the number of arrests that were rape arrests, the insurance premiums, and the percent of drivers involved in fatal accidents that were speeding,and I got the average values across all 50 states for these 3 categories.</p>
<pre class="r"><code>murderstat &lt;- cut((full_mutate$Murder), breaks = c(0, 10, 20), labels = c(&quot;low&quot;, &quot;high&quot;))</code></pre>
<p>Here, I created a new column indicating whether each state's murder arrest value was high or low, defining high as any value above 10.</p>
<pre class="r"><code>full_mutate &lt;- full_mutate %&gt;% mutate(murderstat = murderstat)
full_mutate &lt;- full_mutate %&gt;% mutate (pop_size = ifelse(UrbanPop&lt;50, &quot;small&quot;, &quot;large&quot;))</code></pre>
<p>Here, I added the high/low indication of murder arrest statistics to my dataset and added another categorical column indicating whether the percent of the population that was urban was a small percent or a large percent--larger percents indicating larger urban populations.</p>
<pre class="r"><code>full_mutate %&gt;% group_by(murderstat) %&gt;% summarize(max(Rape), min(Rape))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   murderstat `max(Rape)` `min(Rape)`
##   &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt;
## 1 low               44.5         7.3
## 2 high              46          16.1</code></pre>
<p>Here I grouped my data to look at the groups with high murder stats and low murder stats, and looked at the max and minimum amount of Rape arrests made in each group.</p>
<pre class="r"><code>full_mutate %&gt;% select(Murder:net_ins_profit) %&gt;% summarize_all(sd)</code></pre>
<pre><code>## # A tibble: 1 x 12
##   Murder Assault UrbanPop  Rape perc_alcohol perc_not_distra… perc_no_previous
##    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1   4.36    83.3     14.5  9.37         5.16             15.2             6.84
## # … with 5 more variables: insurance_premiums &lt;dbl&gt;, losses &lt;dbl&gt;,
## #   amt_drivers &lt;dbl&gt;, percent_speeding &lt;dbl&gt;, net_ins_profit &lt;dbl&gt;</code></pre>
<p>Here I calculated the standard deviations for each of my numeric columns.</p>
<pre class="r"><code>full_mutate %&gt;% group_by(pop_size) %&gt;% summarize(number_of_states = n(),not_distract_var = var(perc_not_distracted))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   pop_size number_of_states not_distract_var
##   &lt;chr&gt;               &lt;int&gt;            &lt;dbl&gt;
## 1 large                  42             117.
## 2 small                   8             926</code></pre>
<p>Here, I looked at the two groups of urban population size and calculated the number of states within each group and the variance within each group of the percentage of inidividuals involved in fatal collisions who were not distracted. We find that amongst the 42 states with large urban populations, on average, the variance is 117 for those with large urban populations and 926 for those with small urban populations. Since the small urban population subset only consists of 8 states, this is a rather small sample size allowing for the possibility of extreme values to skew the data which could be why the variance value is much larger than that of the large dataset.</p>
<pre class="r"><code>full_mutate %&gt;% group_by(pop_size, murderstat) %&gt;% summarize(mean(amt_drivers))</code></pre>
<pre><code>## `summarise()` regrouping output by &#39;pop_size&#39; (override with `.groups` argument)</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   pop_size [2]
##   pop_size murderstat `mean(amt_drivers)`
##   &lt;chr&gt;    &lt;fct&gt;                    &lt;dbl&gt;
## 1 large    low                       14.9
## 2 large    high                      16.4
## 3 small    low                       19.8
## 4 small    high                      19.4</code></pre>
<p>Here we see that of those states with a small percentage of urban populations, the average amount of drivers involved in fatal collisions per billion miles are approximately the same across their murder statistics, while of those states with larger urban population percents, there is a small difference, with more drivers involved in fatal collisions in those states with higher murder arrest statistics. Once again, we need to consider our sample size in our small urban population group which could account for the lack of strong distinction.</p>
<pre class="r"><code>library(kableExtra)</code></pre>
<pre><code>## 
## Attaching package: &#39;kableExtra&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     group_rows</code></pre>
<pre class="r"><code>options(knitr.table.format = &quot;html&quot;)

kable_data &lt;- full_mutate %&gt;% group_by(pop_size) %&gt;% summarize(number_of_states = n(), losses_var = var(losses), max(Murder), min(Murder))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>kable_data_states &lt;- full_mutate %&gt;% group_by(murderstat) %&gt;% summarize(max(Rape), max(Assault), max(UrbanPop))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>kable_data %&gt;% kbl() %&gt;% kable_styling()</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
pop_size
</th>
<th style="text-align:right;">
number_of_states
</th>
<th style="text-align:right;">
losses_var
</th>
<th style="text-align:right;">
max(Murder)
</th>
<th style="text-align:right;">
min(Murder)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
large
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
656.3957
</td>
<td style="text-align:right;">
17.4
</td>
<td style="text-align:right;">
2.1
</td>
</tr>
<tr>
<td style="text-align:left;">
small
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
447.2442
</td>
<td style="text-align:right;">
16.1
</td>
<td style="text-align:right;">
0.8
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>kable_data_states %&gt;% kbl() %&gt;% kable_styling()</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
murderstat
</th>
<th style="text-align:right;">
max(Rape)
</th>
<th style="text-align:right;">
max(Assault)
</th>
<th style="text-align:right;">
max(UrbanPop)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
low
</td>
<td style="text-align:right;">
44.5
</td>
<td style="text-align:right;">
294
</td>
<td style="text-align:right;">
91
</td>
</tr>
<tr>
<td style="text-align:left;">
high
</td>
<td style="text-align:right;">
46.0
</td>
<td style="text-align:right;">
337
</td>
<td style="text-align:right;">
86
</td>
</tr>
</tbody>
</table>
<p>Here I have combined some of my summary statistics into a clean table to see. The first table the summary stats for the subgroup of large vs. small urban population size. It shows the number of states in each, along with variance among the losses incurred by car insurance companies due to car accidents, and the maximum and minimum murder arrest for the subgroups. From this table we can that a majority of the states have a large percent urban population and that compared to the states with smaller urban population percents, the murder arrest values are fairly similar, but a large difference in variation across insurance company losses, probably due to the difference in subset group size.<br />
The second table shows summary stats for the subgroup of high vs. low percentage of murder arrest stats. It shows that of those states with high murder arrests, the max amount of Rape arrests was about 44, the max amount of assault arrests was 294, and the max percent of the population that was urban was 94%. This table shows that those states with high murder arrests also higher Rape and Assault arrests, but lower percent urban population.</p>
<pre class="r"><code>full_mutate%&gt;%select_if(is.numeric)%&gt;%cor%&gt;%as.data.frame%&gt;%
  rownames_to_column%&gt;%pivot_longer(-1)%&gt;%
  ggplot(aes(rowname,name,fill=value))+geom_tile()+
  geom_text(size=2.5, aes(label=round(value,2)))+
  xlab(&quot;&quot;)+ylab(&quot;&quot;)+coord_fixed()+
  scale_fill_gradient2(low=&quot;white&quot;,mid=&quot;pink&quot;,high=&quot;red&quot;)+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5))</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>#correlation matrix

ggplot(full_mutate, aes(x=UrbanPop, y=Rape, fill= pop_size))+
  geom_point(aes(color=pop_size, size=pop_size))+
  scale_size_discrete(&quot;pop_size&quot;,range=c(4,2))+
  labs(x= &quot;Percent Urban Population&quot;, y= &quot;Number of Rape Arrests (per 100,000 people)&quot;,
       title=&quot;Rape Arrests vs. Percent Urban Population&quot;)+
  scale_x_continuous(labels = function(x) paste0(x * 1, &#39;%&#39;))+
  scale_x_continuous(breaks = seq(0, 100, 10), limits= c(0,100))</code></pre>
<pre><code>## Warning: Using size for a discrete variable is not advised.</code></pre>
<pre><code>## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will
## replace the existing scale.</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-18-2.png" width="672" /> In this plot we see the relationship between urban populations and the number of rape arrests per 100,000 people. As the percentage of a population's urban population goes up, we see an increase in the number of rape arrests. We see a steep rather than gradual increase in rape arrests as we increase in the percent of a population's urban sector. This could lead us to hypothesize a relationship between urban population and rape arrests. The points in this plot are colored based off of large or small urban population percentage to show the relative sample of size of each, and how each contributes to the correlation.</p>
<pre class="r"><code>ggplot(full_mutate, aes(state))+
  geom_bar(aes(y=net_ins_profit, fill=murderstat), colour=&quot;black&quot;,
           stat=&quot;summary&quot;, fun=mean)+
  theme(axis.text.y = element_text(angle=0, hjust=1, size=5))+
  coord_flip()+
  scale_fill_brewer(palette = &quot;Purples&quot;)+
  labs(y= &quot;Net Insurance Profit&quot;, x= &quot;State&quot;, title= &quot;State vs. Net Insurance Profit&quot;)+
  labs(fill= &quot;Murder Arrests Stats&quot;)</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-19-1.png" width="672" /> In this plot we are looking at the net profit insurance companies made per state after deducting their losses due to auto collisions from their car insurance premiums. The graph is also colored based on whether or not certain states had higher or lower murder arrests. The graph shows that there isn't really a correlatin on whether mostly high/low murder arrests had higher or lower net profits. We can tell that New Jersay is a state with low murder arrests and their car insurance companies have a higher net insurance profit. Ohio is also a state with low murder arrests, but their insurance companies have lower net profits. Also, by looking at the coloring, we can see there is a descrepency in the amount of bars that are colored purple vs. white, indicating and unequal sample for each group. For further research, I would suggest splitting the data into half so that you are looking at groups of 25 and 25 to eliminate the confounding variable of unequal comparison groups.</p>
<pre class="r"><code>#clustering
clust_dat&lt;-full_mutate%&gt;% select(perc_alcohol,perc_no_previous,percent_speeding)

library(cluster)
sil_width&lt;-vector() #empty vector to hold mean sil width
for(i in 2:10){  
  kms &lt;- kmeans(clust_dat,centers=i) 
  sil &lt;- silhouette(kms$cluster,dist(clust_dat)) 
  sil_width[i]&lt;-mean(sil[,3]) 
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name=&quot;k&quot;,breaks=1:10)</code></pre>
<pre><code>## Warning: Removed 1 row(s) containing missing values (geom_path).</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-20-1.png" width="672" /> On the basis of just these three variables, the graph of average silhouette widths shows that 2 clusters is the best (the highest peak!).</p>
<pre class="r"><code>kmeans1 &lt;- clust_dat %&gt;% kmeans(2) 
kmeans1</code></pre>
<pre><code>## K-means clustering with 2 clusters of sizes 32, 18
## 
## Cluster means:
##   perc_alcohol perc_no_previous percent_speeding
## 1     31.62500         89.12500         37.93750
## 2     29.22222         87.38889         20.55556
## 
## Clustering vector:
##  [1] 1 1 1 2 1 1 1 1 2 2 1 1 1 2 2 2 2 1 1 1 2 2 2 2 1 1 2 1 1 2 2 1 1 2 2 1 1 1
## [39] 1 1 1 2 1 1 1 2 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 3054.875 1593.833
##  (between_SS / total_SS =  43.5 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<pre class="r"><code>kmeansclust &lt;- clust_dat %&gt;% mutate(cluster=as.factor(kmeans1$cluster))
kmeansclust %&gt;% ggplot(aes(x= perc_alcohol, y= percent_speeding,color=cluster)) + geom_point()</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-21-1.png" width="672" /> This graph of our clusters shows that the clusters are relatively distinct, but close togeter so the datapoints within the clusters are relatively close together. But within each cluster, we see that the clusters are very spread apart. This shows there is a lot of spread within our clusters, and that there is probably greater distance between the average datapoint and other points within each cluster.</p>
<pre class="r"><code>library(GGally)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre class="r"><code>ggpairs(kmeansclust, columns=1:4, aes(color=cluster))</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-22-1.png" width="672" /> This graph shows how each variable correlates with each other. We can see all the correlation values around 0.2 or 0 yielding a pretty weak correlation for any two variables. When looking at percent of drivers who got into fatal collisions who had not had previous accidents and those who were alcohol-impaired, we see a negative correlation, meaning that as the percent of drivers involved in fatal accidents that were alcohol-impaired increase, the percent of drivers involved in fatal collisions but not previously involved in any previous accidents, decreases. Although we see this negative correlation, the correlation is relatively weak.</p>
<pre class="r"><code>library(cluster)
pam1 &lt;- clust_dat %&gt;% pam(k=2)
pam1  </code></pre>
<pre><code>## Medoids:
##      ID perc_alcohol perc_no_previous percent_speeding
## [1,]  5           28               89               35
## [2,] 46           27               88               19
## Clustering vector:
##  [1] 1 1 1 2 1 1 1 1 2 2 1 1 1 2 2 2 2 1 1 1 2 2 2 2 1 1 2 1 1 2 2 1 1 2 1 1 1 1
## [39] 1 1 1 2 1 1 1 2 1 1 1 1
## Objective function:
##    build     swap 
## 9.745336 9.491604 
## 
## Available components:
##  [1] &quot;medoids&quot;    &quot;id.med&quot;     &quot;clustering&quot; &quot;objective&quot;  &quot;isolation&quot; 
##  [6] &quot;clusinfo&quot;   &quot;silinfo&quot;    &quot;diss&quot;       &quot;call&quot;       &quot;data&quot;</code></pre>
<pre class="r"><code>pamclust&lt;-clust_dat %&gt;% mutate(cluster=as.factor(pam1$clustering)) 
pamclust %&gt;% ggplot(aes(perc_alcohol,perc_no_previous,percent_speeding,color=cluster)) + geom_point()</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-23-1.png" width="672" /> With PAM, we see that our 2 clusters overlap and that there isn't really two distinct clusters. This yields me to believe that the two clusters contain relatively similar data and it would be difficult to establish them as two distinct groupings based on these variables.</p>
<pre class="r"><code>pamclust %&gt;% group_by(cluster) %&gt;% summarize_if(is.numeric,mean,na.rm=T)</code></pre>
<pre><code>## # A tibble: 2 x 4
##   cluster perc_alcohol perc_no_previous percent_speeding
##   &lt;fct&gt;          &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1 1               31.7             88.9             37.6
## 2 2               28.9             87.7             20.1</code></pre>
<p>Here we have the means for each cluster based on each variable. As we can see our cluster means for all three variables are very similar, which is probably what yielded our indistinctive cluster graph.</p>
<pre class="r"><code>plot(pam1,which=2)</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-25-1.png" width="672" /> Here we have our average silhouette width from PAM, which is 0.36. This indicates that our structure is weak and could be artificial. This means our clusters based on these variables are not reliable.</p>

            
        <hr>         <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div> 
            </div>
          </div>

   <hr>  <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div> 
        </div>
      </div>
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="../../js/docs.min.js"></script>
<script src="../../js/main.js"></script>

<script src="../../js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
